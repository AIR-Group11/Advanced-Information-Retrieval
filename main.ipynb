{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVnH1uZ2NvdR",
    "outputId": "9d688d2b-51e7-4380-bef8-efe04cd3bca1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Collecting pinecone\n",
      "  Downloading pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2024.12.14)\n",
      "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone)\n",
      "  Downloading pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Downloading pinecone-5.4.2-py3-none-any.whl (427 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m427.3/427.3 kB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m87.5/87.5 kB\u001B[0m \u001B[31m10.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone\n",
      "Successfully installed pinecone-5.4.2 pinecone-plugin-inference-3.1.0 pinecone-plugin-interface-0.0.7\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.12.14)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m244.8/244.8 kB\u001B[0m \u001B[31m8.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.4/85.4 kB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: pinecone-plugin-inference, pinecone-client\n",
      "  Attempting uninstall: pinecone-plugin-inference\n",
      "    Found existing installation: pinecone-plugin-inference 3.1.0\n",
      "    Uninstalling pinecone-plugin-inference-3.1.0:\n",
      "      Successfully uninstalled pinecone-plugin-inference-3.1.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pinecone 5.4.2 requires pinecone-plugin-inference<4.0.0,>=2.0.0, but you have pinecone-plugin-inference 1.1.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install sentence-transformers\n",
    "!pip install transformers\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install pinecone\n",
    "!pip install pinecone-client\n",
    "!pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7KMfYxvoOAUJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import csv\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pinecone import Pinecone, ServerlessSpec, Index\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"pcsk_6u2fzi_RTrRihKCMgSwZ13NHQi3rhZu7kQvDQ86cx3niadZQGj6UL7nKZcDjCU571LeNfw\")\n",
    "\n",
    "# Define index name\n",
    "index_name = \"song-lyrics-index\"\n",
    "\n",
    "# Check if the index exists; create it if it doesn't\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,  # Dimension of your embeddings\n",
    "        metric=\"cosine\",  # Similarity metric\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",  # Cloud provider\n",
    "            region=\"us-east-1\"  # Region\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Use the Index class to connect directly to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "print(\"Pinecone setup complete!\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhea4Xep6e6C",
    "outputId": "139392a6-1cc3-4c1a-af2e-190f0ea5cfe4"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pinecone setup complete!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "naPFLP-SNvdc"
   },
   "outputs": [],
   "source": [
    "def load_lyrics_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def retrieve_top_k_songs_pinecone(query, index, bi_encoder, k=10):\n",
    "    query_embedding = bi_encoder.encode([query], convert_to_tensor=False)\n",
    "    results = index.query(vector=query_embedding.tolist(), top_k=k * 2, include_metadata=True)  # Fetch more to account for duplicates\n",
    "\n",
    "    # Use a set to keep track of unique songs\n",
    "    unique_songs = {}\n",
    "    for match in results.matches:\n",
    "        key = (match[\"metadata\"][\"Title\"], match[\"metadata\"][\"Artist\"])  # Unique identifier\n",
    "        if key not in unique_songs:\n",
    "            unique_songs[key] = {\n",
    "                \"Title\": match[\"metadata\"][\"Title\"],\n",
    "                \"Artist\": match[\"metadata\"][\"Artist\"],\n",
    "                \"Lyric\": match[\"metadata\"][\"Lyric\"],\n",
    "                \"score\": match.score,\n",
    "            }\n",
    "\n",
    "        # Stop once we have the top k unique songs\n",
    "        if len(unique_songs) >= k:\n",
    "            break\n",
    "\n",
    "    # Sort by score and return top k unique songs\n",
    "    return sorted(unique_songs.values(), key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "\n",
    "def preprocess_lyrics(lyrics, min_segment_size=3, max_segments=10):\n",
    "    words = lyrics.split()\n",
    "    total_words = len(words)\n",
    "\n",
    "    if total_words <= min_segment_size:\n",
    "        return [lyrics]\n",
    "\n",
    "    segment_size = max(min_segment_size, math.ceil(total_words / max_segments))\n",
    "    segments = [\" \".join(words[i:i+segment_size]) for i in range(0, total_words, segment_size)]\n",
    "    return segments\n",
    "\n",
    "def create_finetuning_dataset(df, num_queries=2, num_negative_pairs=10, min_segment_size=3, max_segments=10, qrels_path=\"qrels.csv\"):\n",
    "    queries = []\n",
    "    corpus = []\n",
    "\n",
    "    with open(qrels_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as qrels_file:\n",
    "        qrels_writer = csv.DictWriter(qrels_file, fieldnames=[\"_query_id\", \"song_id\", \"score\"])\n",
    "        qrels_writer.writeheader()\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            title = row['Title']\n",
    "            lyrics = row['Lyric']\n",
    "            artist = row['Artist']\n",
    "\n",
    "            # We split the lyrics into multiple parts and then randomly sample queries from it.\n",
    "            segments = preprocess_lyrics(lyrics, min_segment_size, max_segments)\n",
    "            corpus.append({\"_id\": f\"{idx+1}\", \"Title\": title, \"lyrics\": lyrics, \"Artist\": artist})\n",
    "\n",
    "            selected_queries = random.sample(segments, min(len(segments), num_queries))\n",
    "            for query in selected_queries:\n",
    "                query_id = f\"q{len(queries)+1}\"\n",
    "                queries.append({\"_query_id\": query_id, \"query\": query})\n",
    "\n",
    "                # The song of origin for the specific query will have label 1 (meaning the query is relevant for that song).\n",
    "                qrels_writer.writerow({\"_query_id\": query_id, \"song_id\": f\"{idx+1}\", \"score\": 1})\n",
    "                # Due to size limitations, we randomly sample 100 songs to set the label to 0 (meaning the query is not relevant for that song).\n",
    "                negative_song_indices = [i for i in range(len(df)) if i != idx]\n",
    "                negative_samples = random.sample(negative_song_indices, num_negative_pairs)\n",
    "\n",
    "                for neg_idx in negative_samples:\n",
    "                    qrels_writer.writerow({\"_query_id\": query_id, \"song_id\": f\"{neg_idx+1}\", \"score\": 0})\n",
    "\n",
    "    return queries, corpus"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_lyrics(lyrics, chunk_size=100, overlap=50):\n",
    "    \"\"\"\n",
    "    Uses LangChain's RecursiveCharacterTextSplitter to chunk lyrics.\n",
    "\n",
    "    Args:\n",
    "        lyrics (str): The full lyrics as a single string.\n",
    "        chunk_size (int): The maximum number of characters per chunk.\n",
    "        overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of lyric chunks.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=overlap\n",
    "    )\n",
    "    return splitter.split_text(lyrics)\n"
   ],
   "metadata": {
    "id": "E8FpEOKZN-lI"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_and_store_embeddings(data, index, chunk_size=100, overlap=50, batch_size=100):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    bi_encoder = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    rows = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        lyrics = row['Lyric']\n",
    "        title = row['Title']\n",
    "        artist = row['Artist']\n",
    "\n",
    "        # Convert lyrics to string and handle potential NaN values\n",
    "        lyrics = str(lyrics)  # Ensure lyrics is a string\n",
    "        if lyrics.lower() == 'nan':\n",
    "            continue\n",
    "\n",
    "        # Use LangChain chunker\n",
    "        chunks = splitter.split_text(lyrics)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            rows.append((f\"{idx}-{i}\", chunk, title, artist))\n",
    "\n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        batch = rows[i:i+batch_size]\n",
    "\n",
    "        # Extract chunks for embedding\n",
    "        chunks = [row[1] for row in batch]\n",
    "        embeddings = bi_encoder.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "        # Prepare data for upsert\n",
    "        vectors = []\n",
    "        for (vector_id, chunk, title, artist), embedding in zip(batch, embeddings):\n",
    "            metadata = {\n",
    "                \"Title\": title,\n",
    "                \"Artist\": artist,\n",
    "                \"Lyric\": chunk\n",
    "            }\n",
    "            vectors.append((vector_id, embedding.tolist(), metadata))\n",
    "\n",
    "        # Upsert the batch to Pinecone\n",
    "        index.upsert(vectors)\n",
    "        print(f\"Upserted batch {i//batch_size + 1}/{(len(rows) + batch_size - 1) // batch_size}\")\n",
    "\n",
    "    print(\"Embeddings stored in Pinecone!\")\n"
   ],
   "metadata": {
    "id": "Kbip6bwfFTVq"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UPR7hHV0Nvde"
   },
   "outputs": [],
   "source": [
    "class BiEncoder:\n",
    "    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def encode_texts(self, texts):\n",
    "        \"\"\"Used for encoding lyrics into embeddings.\"\"\"\n",
    "        return self.model.encode(texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "class CrossEncoder:\n",
    "    def __init__(self, model_name='cross-encoder/ms-marco-MiniLM-L-6-v2'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def rank_candidates(self, query, candidates):\n",
    "        inputs = [\n",
    "            self.tokenizer(query, candidate, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "            for candidate in candidates\n",
    "        ]\n",
    "        scores = []\n",
    "        for input_pair in inputs:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(**input_pair).logits\n",
    "            scores.append(logits.item())\n",
    "        ranked_indices = np.argsort(scores)[::-1]\n",
    "        return ranked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Hsv8n1c-Nvdg"
   },
   "outputs": [],
   "source": [
    "def song_retrieval_pipeline(query, index, bi_encoder_model='sentence-transformers/all-mpnet-base-v2', k=5):\n",
    "    # Initialize Bi-Encoder\n",
    "    bi_encoder = SentenceTransformer(bi_encoder_model)\n",
    "\n",
    "    # Retrieve top-k unique songs using Pinecone\n",
    "    top_k_songs = retrieve_top_k_songs_pinecone(query, index, bi_encoder, k)\n",
    "\n",
    "    return top_k_songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eHmwrHLSNvdh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97348c7d-0f0a-4988-9cc5-336525012a6b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset obtained.\n",
      "Embeddings already exist in Pinecone.\n",
      "Top retrieved songs:\n",
      "1. Bad romance / Applause / Swine / Gyspy (artRAVE paris version) by Lady Gaga\n",
      "2. Applause - Come To Mama - Edge Of Glory - Born This Way (ACT IV) by Lady Gaga\n",
      "3. Applause / Come To Mama / The edge Of Glory / Born This Way by Lady Gaga\n",
      "4. Applause (Demo) by Lady Gaga\n",
      "5. Applause (Purity Ring Remix) by Lady Gaga\n",
      "6. Applause (Empire of the Sun Remix) by Lady Gaga\n",
      "7. Applause by Lady Gaga\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/dataset.csv\"\n",
    "query = \"I live for the applause\"\n",
    "\n",
    "\n",
    "# Precompute and store embeddings in Pinecone if not already stored\n",
    "if not index.describe_index_stats()[\"total_vector_count\"]:\n",
    "  if os.path.exists(dataset_path):\n",
    "    data = load_lyrics_dataset(dataset_path)\n",
    "    preprocess_and_store_embeddings(data, index, chunk_size=100, overlap=50)  # Use chunking params\n",
    "  else:\n",
    "      print(\"Dataset path does not exist.\")\n",
    "else:\n",
    "  print(\"Embeddings already exist in Pinecone.\")\n",
    "\n",
    "    # Run the song retrieval pipeline\n",
    "results = song_retrieval_pipeline(query, index)\n",
    "print(\"Top retrieved songs:\")\n",
    "for idx, song in enumerate(results):\n",
    "  print(f\"{idx + 1}. {song['Title']} by {song['Artist']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2nshSp8xJIZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
