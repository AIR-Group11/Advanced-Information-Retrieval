{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVnH1uZ2NvdR",
    "outputId": "3ff1c822-1916-4418-824e-fe387912a6b1",
    "ExecuteTime": {
     "end_time": "2025-01-09T20:16:30.414577Z",
     "start_time": "2025-01-09T20:16:10.349579Z"
    }
   },
   "source": [
    "!pip install torch\n",
    "!pip install sentence-transformers\n",
    "!pip install transformers\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install pinecone\n",
    "!pip install pinecone-client\n",
    "!pip install langchain\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.5.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.5)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.0)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (75.6.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.12/site-packages (3.3.1)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.47.1)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\r\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.15.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (0.27.0)\r\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.47.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.16.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.27.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.2.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.2.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.2.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone in ./.venv/lib/python3.12/site-packages (5.4.2)\r\n",
      "Requirement already satisfied: certifi>=2019.11.17 in ./.venv/lib/python3.12/site-packages (from pinecone) (2024.12.14)\r\n",
      "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone)\r\n",
      "  Obtaining dependency information for pinecone-plugin-inference<4.0.0,>=2.0.0 from https://files.pythonhosted.org/packages/89/45/4ae4e38439919584c2d34b6bef5d0ef8d068030871dd4da911d174840ee6/pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata\r\n",
      "  Using cached pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in ./.venv/lib/python3.12/site-packages (from pinecone) (0.0.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.12/site-packages (from pinecone) (2.9.0.post0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.1 in ./.venv/lib/python3.12/site-packages (from pinecone) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in ./.venv/lib/python3.12/site-packages (from pinecone) (4.12.2)\r\n",
      "Requirement already satisfied: urllib3>=1.26.5 in ./.venv/lib/python3.12/site-packages (from pinecone) (2.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\r\n",
      "Using cached pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\r\n",
      "Installing collected packages: pinecone-plugin-inference\r\n",
      "  Attempting uninstall: pinecone-plugin-inference\r\n",
      "    Found existing installation: pinecone-plugin-inference 1.1.0\r\n",
      "    Uninstalling pinecone-plugin-inference-1.1.0:\r\n",
      "      Successfully uninstalled pinecone-plugin-inference-1.1.0\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pinecone-client 5.0.1 requires pinecone-plugin-inference<2.0.0,>=1.0.3, but you have pinecone-plugin-inference 3.1.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed pinecone-plugin-inference-3.1.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in ./.venv/lib/python3.12/site-packages (5.0.1)\r\n",
      "Requirement already satisfied: certifi>=2019.11.17 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (2024.12.14)\r\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\r\n",
      "  Obtaining dependency information for pinecone-plugin-inference<2.0.0,>=1.0.3 from https://files.pythonhosted.org/packages/d8/5e/a7eb453cfb3aa9c8c995a1dca5fcf57f79b67400593d5c6759571567e30c/pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata\r\n",
      "  Using cached pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (0.0.7)\r\n",
      "Requirement already satisfied: tqdm>=4.64.1 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (4.12.2)\r\n",
      "Requirement already satisfied: urllib3>=1.26.5 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (2.3.0)\r\n",
      "Using cached pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\r\n",
      "Installing collected packages: pinecone-plugin-inference\r\n",
      "  Attempting uninstall: pinecone-plugin-inference\r\n",
      "    Found existing installation: pinecone-plugin-inference 3.1.0\r\n",
      "    Uninstalling pinecone-plugin-inference-3.1.0:\r\n",
      "      Successfully uninstalled pinecone-plugin-inference-3.1.0\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pinecone 5.4.2 requires pinecone-plugin-inference<4.0.0,>=2.0.0, but you have pinecone-plugin-inference 1.1.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed pinecone-plugin-inference-1.1.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.12/site-packages (0.3.14)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain) (6.0.2)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain) (2.0.36)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain) (3.11.11)\r\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.29)\r\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.5)\r\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in ./.venv/lib/python3.12/site-packages (from langchain) (0.2.10)\r\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in ./.venv/lib/python3.12/site-packages (from langchain) (2.2.1)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.12/site-packages (from langchain) (2.10.4)\r\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain) (2.32.3)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain) (9.0.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.13)\r\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.12.14)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\r\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.8.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.2.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from datasets) (2024.9.0)\r\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.11)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.27.0)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T20:11:47.878352Z",
     "start_time": "2025-01-09T20:11:17.167926Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\r\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/d7/84/0df6c5981f5fc722381662ff8cfbdf8aad64bec875f75d80b55bfef394ce/datasets-3.2.0-py3-none-any.whl.metadata\r\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.1)\r\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\r\n",
      "  Obtaining dependency information for pyarrow>=15.0.0 from https://files.pythonhosted.org/packages/3a/2e/3b99f8a3d9e0ccae0e961978a0d0089b25fb46ebbcfb5ebae3cca179a5b3/pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata\r\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\r\n",
      "  Obtaining dependency information for dill<0.3.9,>=0.3.0 from https://files.pythonhosted.org/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl.metadata\r\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\r\n",
      "Collecting xxhash (from datasets)\r\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/11/a7/81dba5010f7e733de88af9555725146fc133be97ce36533867f4c7e75066/xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Collecting multiprocess<0.70.17 (from datasets)\r\n",
      "  Obtaining dependency information for multiprocess<0.70.17 from https://files.pythonhosted.org/packages/0a/7d/a988f258104dcd2ccf1ed40fdc97e26c4ac351eeaf81d76e266c52d84e2f/multiprocess-0.70.16-py312-none-any.whl.metadata\r\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0 (from datasets)\r\n",
      "  Obtaining dependency information for fsspec[http]<=2024.9.0,>=2023.1.0 from https://files.pythonhosted.org/packages/1d/a0/6aaea0c2fbea2f89bfd5db25fb1e3481896a423002ebe4e55288907a97a3/fsspec-2024.9.0-py3-none-any.whl.metadata\r\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.11)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.27.0)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m480.6/480.6 kB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m146.7/146.7 kB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.1/40.1 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m0m\r\n",
      "\u001B[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.4/194.4 kB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m179.3/179.3 kB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: xxhash, pyarrow, fsspec, dill, multiprocess, datasets\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2024.12.0\r\n",
      "    Uninstalling fsspec-2024.12.0:\r\n",
      "      Successfully uninstalled fsspec-2024.12.0\r\n",
      "Successfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 pyarrow-18.1.0 xxhash-3.5.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7KMfYxvoOAUJ",
    "ExecuteTime": {
     "end_time": "2025-01-10T10:29:44.344107Z",
     "start_time": "2025-01-10T10:29:31.988408Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import csv\n",
    "import random\n",
    "from openai import OpenAI\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksim/PycharmProjects/Advanced-Information-Retrieval/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Cng6SewPXDQP",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:36:32.437972Z",
     "start_time": "2025-01-09T18:36:32.370895Z"
    }
   },
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'api_key'\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhea4Xep6e6C",
    "outputId": "d28b5598-7b14-4137-e621-878f0ee36019",
    "ExecuteTime": {
     "end_time": "2025-01-10T10:38:08.028100Z",
     "start_time": "2025-01-10T10:38:07.412061Z"
    }
   },
   "source": [
    "from pinecone import Pinecone, ServerlessSpec, Index\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"api_key\")\n",
    "\n",
    "index_name = \"air\"\n",
    "\n",
    "# Check if the index exists; create it if it doesn't\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,  # Dimension of your embeddings\n",
    "        metric=\"cosine\",  # Similarity metric\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "print(\"Pinecone setup complete!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone setup complete!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "naPFLP-SNvdc",
    "ExecuteTime": {
     "end_time": "2025-01-10T10:38:30.584291Z",
     "start_time": "2025-01-10T10:38:30.570668Z"
    }
   },
   "source": [
    "\n",
    "def load_lyrics_dataset(file_path):\n",
    "    try:\n",
    "        # Attempt to read the file with 'latin1' encoding\n",
    "        df = pd.read_csv(file_path, encoding='latin1', on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Preprocessing\n",
    "    # Remove rows with missing values in key columns\n",
    "    df = df.dropna(subset=['Artist', 'Song', 'Lyrics'])\n",
    "    # Remove rows where lyrics have fewer than 3 words\n",
    "    df['lyrics_word_count'] = df['Lyrics'].apply(lambda x: len(str(x).split()))\n",
    "    df = df[df['lyrics_word_count'] >= 3]\n",
    "\n",
    "    df = df.drop(columns=['lyrics_word_count'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# def retrieve_top_k_songs_pinecone(query, index, bi_encoder, k=10):\n",
    "#     query_embedding = bi_encoder.encode([query], convert_to_tensor=False)\n",
    "#     results = index.query(vector=query_embedding.tolist(), top_k=k * 2, include_metadata=True)  # Fetch more to account for duplicates\n",
    "\n",
    "#     # Use a set to keep track of unique songs\n",
    "#     unique_songs = {}\n",
    "#     for match in results.matches:\n",
    "#         key = (match[\"metadata\"][\"Song\"], match[\"metadata\"][\"Artist\"])  # Unique identifier\n",
    "#         if key not in unique_songs:\n",
    "#             unique_songs[key] = {\n",
    "#                 \"Song\": match[\"metadata\"][\"Song\"],\n",
    "#                 \"Artist\": match[\"metadata\"][\"Artist\"],\n",
    "#                 \"Lyric\": match[\"metadata\"][\"Lyric\"],\n",
    "#                 \"score\": match.score,\n",
    "#             }\n",
    "\n",
    "#         # Stop once we have the top k unique songs\n",
    "#         if len(unique_songs) >= k:\n",
    "#             break\n",
    "\n",
    "#     # Sort by score and return top k unique songs\n",
    "#     return sorted(unique_songs.values(), key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "\n",
    "def preprocess_lyrics(lyrics, min_segment_size=3, max_segments=10):\n",
    "    words = lyrics.split()\n",
    "    total_words = len(words)\n",
    "\n",
    "    if total_words <= min_segment_size:\n",
    "        return [lyrics]\n",
    "\n",
    "    segment_size = max(min_segment_size, math.ceil(total_words / max_segments))\n",
    "    segments = [\" \".join(words[i:i+segment_size]) for i in range(0, total_words, segment_size)]\n",
    "    return segments\n",
    "\n",
    "def prepare_embedding_training_data(input_csv_path, output_csv_path, queries_per_song=5, window_size=10, step=5):\n",
    "    df = pd.read_csv(input_csv_path, encoding='latin1')\n",
    "\n",
    "    # Convert Non-String Values to Strings\n",
    "    df[\"Lyrics\"] = df[\"Lyrics\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Verify required columns exist\n",
    "    required_columns = [\"Song\", \"Artist\", \"Lyrics\"]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Store query-lyric pairs with labels\n",
    "    data = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        full_lyrics = row[\"Lyrics\"]\n",
    "\n",
    "        # Use the row index as a unique song ID\n",
    "        song_id = index\n",
    "\n",
    "        # Split lyrics into words\n",
    "        words = full_lyrics.split()\n",
    "\n",
    "        # Generate positive queries (labels = 1.0)\n",
    "        for start in range(0, len(words) - window_size + 1, step):\n",
    "            query = \" \".join(words[start:start + window_size])\n",
    "            data.append({\n",
    "                \"query\": query,\n",
    "                \"song_id\": song_id,\n",
    "                \"full_lyrics\": full_lyrics,\n",
    "                \"label\": 1.0\n",
    "            })\n",
    "\n",
    "        # Generate negative queries (labels = 0.0)\n",
    "        negative_samples = random.sample(df[df.index != index].to_dict(orient=\"records\"), k=queries_per_song)\n",
    "        for neg_sample in negative_samples:\n",
    "            data.append({\n",
    "                \"query\": query,\n",
    "                \"song_id\": neg_sample[\"Song\"],\n",
    "                \"full_lyrics\": neg_sample[\"Lyrics\"],\n",
    "                \"label\": 0.0\n",
    "            })\n",
    "\n",
    "    # Save the prepared dataset as a CSV file\n",
    "    processed_df = pd.DataFrame(data)\n",
    "    processed_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Dataset prepared and saved to {output_csv_path}\")\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:57:09.577621Z",
     "start_time": "2025-01-09T21:55:54.490281Z"
    }
   },
   "cell_type": "code",
   "source": "prepare_embedding_training_data(input_csv_path=\"/home/maksim/PycharmProjects/Advanced-Information-Retrieval/data/dataset.csv\", output_csv_path=\"/home/maksim/PycharmProjects/Advanced-Information-Retrieval/data/output_dataset.csv\")\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared and saved to /home/maksim/PycharmProjects/Advanced-Information-Retrieval/data/output_dataset.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T20:35:58.745958Z",
     "start_time": "2025-01-09T20:35:24.590860Z"
    }
   },
   "cell_type": "code",
   "source": "finetune_embedding_model(\"/home/maksim/PycharmProjects/Advanced-Information-Retrieval/data/output_dataset.csv\")\n",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 339317/339317 [00:21<00:00, 15609.77 examples/s]\n",
      "/home/maksim/PycharmProjects/Advanced-Information-Retrieval/.venv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[70], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mfinetune_embedding_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/maksim/PycharmProjects/Advanced-Information-Retrieval/data/output_dataset.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[66], line 173\u001B[0m, in \u001B[0;36mfinetune_embedding_model\u001B[0;34m(dataset_path, model_name, output_model_path, batch_size, epochs)\u001B[0m\n\u001B[1;32m    170\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m losses\u001B[38;5;241m.\u001B[39mCosineSimilarityLoss(model)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# Set up training arguments\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m training_args \u001B[38;5;241m=\u001B[39m \u001B[43mSentenceTransformerTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_model_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msteps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43moutput_model_path\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/logs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_total_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremove_unused_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpush_to_hub\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;66;03m# Initialize the trainer\u001B[39;00m\n\u001B[1;32m    189\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SentenceTransformerTrainer(\n\u001B[1;32m    190\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    191\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m    192\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mdataset,\n\u001B[1;32m    193\u001B[0m     loss\u001B[38;5;241m=\u001B[39mtrain_loss,\n\u001B[1;32m    194\u001B[0m )\n",
      "File \u001B[0;32m<string>:137\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices, prompts, batch_sampler, multi_dataset_batch_sampler)\u001B[0m\n",
      "File \u001B[0;32m~/PycharmProjects/Advanced-Information-Retrieval/.venv/lib/python3.12/site-packages/sentence_transformers/training_args.py:182\u001B[0m, in \u001B[0;36mSentenceTransformerTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__post_init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 182\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__post_init__\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_sampler \u001B[38;5;241m=\u001B[39m BatchSamplers(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_sampler)\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmulti_dataset_batch_sampler \u001B[38;5;241m=\u001B[39m MultiDatasetBatchSamplers(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmulti_dataset_batch_sampler)\n",
      "File \u001B[0;32m~/PycharmProjects/Advanced-Information-Retrieval/.venv/lib/python3.12/site-packages/transformers/training_args.py:1780\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1778\u001B[0m \u001B[38;5;66;03m# Initialize device before we proceed\u001B[39;00m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_torch_available():\n\u001B[0;32m-> 1780\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m   1782\u001B[0m \u001B[38;5;66;03m# Disable average tokens when using single device\u001B[39;00m\n\u001B[1;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_tokens_across_devices:\n",
      "File \u001B[0;32m~/PycharmProjects/Advanced-Information-Retrieval/.venv/lib/python3.12/site-packages/transformers/training_args.py:2306\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2302\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2303\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[1;32m   2304\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2305\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m-> 2306\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Advanced-Information-Retrieval/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:60\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, obj, objtype)\u001B[0m\n\u001B[1;32m     58\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 60\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "File \u001B[0;32m~/PycharmProjects/Advanced-Information-Retrieval/.venv/lib/python3.12/site-packages/transformers/training_args.py:2179\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2177\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[1;32m   2178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[0;32m-> 2179\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m   2180\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2181\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccelerate>=\u001B[39m\u001B[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2182\u001B[0m         )\n\u001B[1;32m   2183\u001B[0m \u001B[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001B[39;00m\n\u001B[1;32m   2184\u001B[0m accelerator_state_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menabled\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_configured_state\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n",
      "\u001B[0;31mImportError\u001B[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E8FpEOKZN-lI",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:37:18.557024Z",
     "start_time": "2025-01-09T18:37:18.261217Z"
    }
   },
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_lyrics(lyrics, chunk_size=100, overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=overlap\n",
    "    )\n",
    "    return splitter.split_text(lyrics)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kbip6bwfFTVq",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:37:33.167526Z",
     "start_time": "2025-01-09T18:37:33.158021Z"
    }
   },
   "source": [
    "def preprocess_and_store_embeddings(data, index, chunk_size=100, overlap=50, batch_size=100):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    bi_encoder = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    rows = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        lyrics = row['Lyrics']\n",
    "        Song = row['Song']\n",
    "        artist = row['Artist']\n",
    "\n",
    "        # Convert lyrics to string and handle potential NaN values\n",
    "        lyrics = str(lyrics)  # Ensure lyrics is a string\n",
    "        if lyrics.lower() == 'nan':\n",
    "            continue\n",
    "\n",
    "        # Use LangChain chunker\n",
    "        chunks = splitter.split_text(lyrics)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            rows.append((f\"{idx}-{i}\", chunk, Song, artist))\n",
    "\n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        batch = rows[i:i+batch_size]\n",
    "\n",
    "        # Extract chunks for embedding\n",
    "        chunks = [row[1] for row in batch]\n",
    "        embeddings = bi_encoder.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "        # Prepare data for upsert\n",
    "        vectors = []\n",
    "        for (vector_id, chunk, Song, artist), embedding in zip(batch, embeddings):\n",
    "            metadata = {\n",
    "                \"Song\": Song,\n",
    "                \"Artist\": artist,\n",
    "                \"Lyrics\": chunk\n",
    "            }\n",
    "            vectors.append((vector_id, embedding.tolist(), metadata))\n",
    "\n",
    "        # Upsert the batch to Pinecone\n",
    "        index.upsert(vectors)\n",
    "        print(f\"Upserted batch {i//batch_size + 1}/{(len(rows) + batch_size - 1) // batch_size}\")\n",
    "\n",
    "    print(\"Embeddings stored in Pinecone!\")\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UPR7hHV0Nvde",
    "ExecuteTime": {
     "end_time": "2025-01-09T19:17:33.456734Z",
     "start_time": "2025-01-09T19:17:33.450507Z"
    }
   },
   "source": [
    "class BiEncoder:\n",
    "    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def encode_texts(self, texts):\n",
    "        \"\"\"Used for encoding lyrics into embeddings.\"\"\"\n",
    "        return self.model.encode(texts, convert_to_tensor=True, show_progress_bar=True)\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UbcthLP0WCjj",
    "ExecuteTime": {
     "end_time": "2025-01-09T19:02:56.372911Z",
     "start_time": "2025-01-09T19:02:56.366972Z"
    }
   },
   "source": [
    "def refine_query_with_chatgpt(query):\n",
    "    # Few-shot examples to guide the model\n",
    "    example_message = (\n",
    "        \"You are a helpful assistant tasked with extracting and correcting song lyrics \"\n",
    "        \"from user input and identifying the author if mentioned. You should return only the \"\n",
    "        \"lyric portion of the query and refine it for correctness. Format your output as follows:\\n\"\n",
    "        \"query: <start_query>original_query<end_query>\\n\"\n",
    "        \"refined_query: <start_refined_query>refined_lyric<end_refined_query>\\n\"\n",
    "        \"author: <start_author>author_name<end_author>\\n\"\n",
    "        \"If the author is not mentioned, return 'NOT_MENTIONED' for the author.\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"1. User input: \\\"What's the song from Rihanna that goes like 'how about a round of applause'\\\"\\n\"\n",
    "        \"   Output:\\n\"\n",
    "        \"   query: <start_query>how about a round of applause<end_query>\\n\"\n",
    "        \"   refined_query: <start_refined_query>How about a round of applause<end_refined_query>\\n\"\n",
    "        \"   author: <start_author>Rihanna<end_author>\\n\\n\"\n",
    "        \"2. User input: \\\"Twinkle twinkle litl star how I wondr wht u ar\\\"\\n\"\n",
    "        \"   Output:\\n\"\n",
    "        \"   query: <start_query>Twinkle twinkle litl star how I wondr wht u ar<end_query>\\n\"\n",
    "        \"   refined_query: <start_refined_query>Twinkle twinkle little star how I wonder what you are<end_refined_query>\\n\"\n",
    "        \"   author: <start_author>NOT_MENTIONED<end_author>\\n\\n\"\n",
    "        \"3. User input: \\\"song about a broken heart\\\"\\n\"\n",
    "        \"   Output:\\n\"\n",
    "        \"   query: <start_query>a broken heart<end_query>\\n\"\n",
    "        \"   refined_query: <start_refined_query>A broken heart<end_refined_query>\\n\"\n",
    "        \"   author: <start_author>NOT_MENTIONED<end_author>\\n\\n\"\n",
    "        \"Now process this input:\\n\"\n",
    "        f\"User input: {query}\"\n",
    "    )\n",
    "\n",
    "    # Call the OpenAI API\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant tasked with extracting and correcting song lyrics from user input and identifying the author if mentioned.\"},\n",
    "            {\"role\": \"user\", \"content\": example_message},\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    # Extract and format the result\n",
    "    output = chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "    # Manually parse the refined query from the output\n",
    "    start_tag = \"<start_refined_query>\"\n",
    "    end_tag = \"<end_refined_query>\"\n",
    "    refined_query = \"\"\n",
    "    if start_tag in output and end_tag in output:\n",
    "        refined_query = output.split(start_tag)[-1].split(end_tag)[0].strip()\n",
    "\n",
    "    print(\"Refined query:\\n\", refined_query)\n",
    "    return refined_query\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381,
     "referenced_widgets": [
      "04d18beae8ff459fb2d41be9190d3769",
      "115878b8291e427b9fb615e330c24276",
      "98243bdfd0be4c9b9029aab54d5f48b1",
      "fe7d0737f6fb470198f4b3977dce2392",
      "e2ee0b6b082a407b86c8872d6c5de9fa",
      "544e0369b1d947a3ac9625fb62ec8120",
      "58c826569e834b9885f57f120bf38c91",
      "04cdc086b4e84c22b714f29925be7812",
      "77ed47b807454a48bbbd9781fa7e0195",
      "06cd16b3378d4effb047b6f0895da170",
      "f61ba415507f4c70b40dd26b4443f0b1"
     ]
    },
    "id": "Hsv8n1c-Nvdg",
    "outputId": "6b0bbe66-025b-4b03-b290-e04949c7992f",
    "ExecuteTime": {
     "end_time": "2025-01-09T19:17:52.925902Z",
     "start_time": "2025-01-09T19:17:52.913262Z"
    }
   },
   "source": [
    "def song_retrieval_pipeline(query, index, bi_encoder_model='sentence-transformers/all-mpnet-base-v2',\n",
    "                            cross_encoder_model='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "                            bi_encoder_k=100, cross_encoder_k=10):\n",
    "    # Bi-Encoder\n",
    "    bi_encoder = BiEncoder(bi_encoder_model)\n",
    "\n",
    "    refined_query = refine_query_with_chatgpt(query)\n",
    "    query_embedding = bi_encoder.encode_texts([refined_query])[0].tolist()\n",
    "\n",
    "    # Query Pinecone\n",
    "    results = index.query(vector=query_embedding, top_k=bi_encoder_k, include_metadata=True)\n",
    "\n",
    "    # Initial retrieval with duplicate removal\n",
    "    candidates = []\n",
    "    seen_songs = set()\n",
    "\n",
    "    for match in results[\"matches\"]:\n",
    "        song_key = (match[\"metadata\"][\"Song\"], match[\"metadata\"][\"Artist\"])\n",
    "        if song_key not in seen_songs:\n",
    "            seen_songs.add(song_key)\n",
    "            candidates.append({\n",
    "                \"track_name\": match[\"metadata\"][\"Song\"],\n",
    "                \"artist_name\": match[\"metadata\"][\"Artist\"],\n",
    "                \"lyrics_chunk\": match[\"metadata\"][\"Lyrics\"],\n",
    "                \"score\": match.score\n",
    "            })\n",
    "            if len(candidates) >= bi_encoder_k:  # Stop if we have enough unique songs\n",
    "                break\n",
    "\n",
    "    print(\"Top 10 results before reranking:\")\n",
    "    for candidate in candidates[:10]:\n",
    "        print(f\"Track: {candidate['track_name']}, Artist: {candidate['artist_name']}, Score: {candidate['score']}\")\n",
    "\n",
    "    candidates = sorted(candidates, key=lambda x: x['score'], reverse=True)[:cross_encoder_k]\n",
    "    lyrics_chunks = [candidate[\"lyrics_chunk\"] for candidate in candidates]\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    # Cross-Encoder\n",
    "    cross_encoder = CrossEncoder(model_name=cross_encoder_model)\n",
    "    candidate_pairs = [(query, chunk) for chunk in lyrics_chunks]\n",
    "    scores = cross_encoder.predict(candidate_pairs, batch_size=16)\n",
    "\n",
    "    # Re-ranking\n",
    "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    re_ranked_songs = [candidates[i] for i in ranked_indices]\n",
    "\n",
    "    print(\"\\nTop 10 results after reranking:\")\n",
    "    for song in re_ranked_songs[:10]:\n",
    "        print(f\"Track: {song['track_name']}, Artist: {song['artist_name']}, Score: {song['score']}\")\n",
    "\n",
    "    return re_ranked_songs\n",
    "\n",
    "#song_retrieval_pipeline(\"something about an applouse\", index)\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "9fd1b7427e56400fb098037a39cef39a",
      "4825fa9172a74ecd9eefa20c4ae92dfd",
      "66e4430fcc034d898adb6cdead5c0eca",
      "0224bf5043d84158a467780c1b9ee6ac",
      "95a400c2c9c44470940e8d1dfa034602",
      "a9b91de5957c4ad091c7109d5ec89253",
      "486c63fe71e548499e174f5ef63994dc",
      "83a957d95ad7486485be2e9c51b4b640",
      "2528953881b84d239a4931958e46099c",
      "c122fd1ea658494cb2712fb2a6afd21d",
      "2896688d51694dc28b0c35b5c1392d52"
     ]
    },
    "id": "eHmwrHLSNvdh",
    "outputId": "222291d8-8c23-4aaa-d73d-f6be085be7f0",
    "ExecuteTime": {
     "end_time": "2025-01-09T19:18:01.501900Z",
     "start_time": "2025-01-09T19:17:56.231059Z"
    }
   },
   "source": [
    "# Main program\n",
    "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/dataset.csv\"\n",
    "corpus_path = \"/content/drive/MyDrive/Colab Notebooks/corpus.csv\"\n",
    "queries_path = \"/content/drive/MyDrive/Colab Notebooks/queries.csv\"\n",
    "qrels_path = \"/content/drive/MyDrive/Colab Notebooks/qrels.csv\"\n",
    "query = \"songs about broken heart\"\n",
    "\n",
    "if not index.describe_index_stats()[\"total_vector_count\"]:\n",
    "\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(\"Dataset obtained.\")\n",
    "        data = load_lyrics_dataset(dataset_path)\n",
    "\n",
    "        if not (os.path.exists(corpus_path) and os.path.exists(queries_path) and os.path.exists(qrels_path)):\n",
    "            print(\"Required files not found. Generating fine-tuning datasets...\")\n",
    "\n",
    "            # Generate fine-tuning datasets\n",
    "            queries, corpus = create_finetuning_dataset(data, num_queries=5, num_negative_pairs=100, qrels_path=qrels_path)\n",
    "            preprocess_and_store_embeddings(data, index, chunk_size=100, overlap=50)  # Use chunking params\n",
    "\n",
    "            # Save queries and corpus to CSV\n",
    "            queries_df = pd.DataFrame(queries)\n",
    "            corpus_df = pd.DataFrame(corpus)\n",
    "            queries_df.to_csv(queries_path, index=False)\n",
    "            corpus_df.to_csv(corpus_path, index=False)\n",
    "            print(\"Datasets generated and saved.\")\n",
    "        else:\n",
    "            print(\"Datasets already exist. Skipping dataset generation.\")\n",
    "\n",
    "results = song_retrieval_pipeline(query, index)\n",
    "print(\"Top retrieved songs:\")\n",
    "for idx, song in enumerate(results):\n",
    "    print(f\"{idx + 1}. {song['track_name']} by {song['artist_name']}\")\n",
    "    print(f\"Lyrics Chunk: {song['lyrics_chunk']}\\n\")\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined query:\n",
      " Broken heart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 results before reranking:\n",
      "Track: cross my broken heart, Artist: the jets, Score: 0.630817056\n",
      "Track: brokenhearted, Artist: brandy featuring wanya morris, Score: 0.628032863\n",
      "Track: two to make it right, Artist: seduction, Score: 0.61804378\n",
      "Track: breakeven, Artist: the script, Score: 0.601431489\n",
      "Track: luv u better, Artist: ll cool j, Score: 0.601056\n",
      "Track: time love and tenderness, Artist: michael bolton, Score: 0.60098505\n",
      "Track: total eclipse of the heart, Artist: nicki french, Score: 0.599912345\n",
      "Track: im still in love with you, Artist: sean paul featuring sasha, Score: 0.598965168\n",
      "Track: second chance, Artist: 38 special, Score: 0.598877311\n",
      "Track: ill never fall in love again, Artist: tom jones, Score: 0.592734754\n",
      "\n",
      "Top 10 results after reranking:\n",
      "Track: cross my broken heart, Artist: the jets, Score: 0.630817056\n",
      "Track: im still in love with you, Artist: sean paul featuring sasha, Score: 0.598965168\n",
      "Track: time love and tenderness, Artist: michael bolton, Score: 0.60098505\n",
      "Track: breakeven, Artist: the script, Score: 0.601431489\n",
      "Track: two to make it right, Artist: seduction, Score: 0.61804378\n",
      "Track: brokenhearted, Artist: brandy featuring wanya morris, Score: 0.628032863\n",
      "Track: luv u better, Artist: ll cool j, Score: 0.601056\n",
      "Track: ill never fall in love again, Artist: tom jones, Score: 0.592734754\n",
      "Track: total eclipse of the heart, Artist: nicki french, Score: 0.599912345\n",
      "Track: second chance, Artist: 38 special, Score: 0.598877311\n",
      "Top retrieved songs:\n",
      "1. cross my broken heart by the jets\n",
      "Lyrics Chunk: broken heart boy cross my broken heart for you im going to stay this timecross my broken heart boy\n",
      "\n",
      "2. im still in love with you by sean paul featuring sasha\n",
      "Lyrics Chunk: well in come a ting dem call a broken heart dis blessed love will neva part it dont know it from da\n",
      "\n",
      "3. time love and tenderness by michael bolton\n",
      "Lyrics Chunk: nothing cures a broken heart like time love and tenderness\n",
      "\n",
      "4. breakeven by the script\n",
      "Lyrics Chunk: when a heart breaks no it dont breakeven even no what am i gonna do when the best part of me was\n",
      "\n",
      "5. two to make it right by seduction\n",
      "Lyrics Chunk: youre suffering from a serious broken heart you know that its a shame but im here to ease your pain\n",
      "\n",
      "6. brokenhearted by brandy featuring wanya morris\n",
      "Lyrics Chunk: brokenhearted its a hurting thing to get over starting all over againonly brokenhearted lifes not\n",
      "\n",
      "7. luv u better by ll cool j\n",
      "Lyrics Chunk: hearts hairbroken and thats hard to fix i had to dig deep inside myself cuz i cant see you bouncin\n",
      "\n",
      "8. ill never fall in love again by tom jones\n",
      "Lyrics Chunk: for someone else baby i broke up all insideand it looks like im never gonna fall in love again\n",
      "\n",
      "9. total eclipse of the heart by nicki french\n",
      "Lyrics Chunk: love now im only falling apart theres nothing i can do a total eclipse of the heart turnaround\n",
      "\n",
      "10. second chance by 38 special\n",
      "Lyrics Chunk: heart needs a second chance when love makes this sound babe a heart needs a second chance dont put\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2nshSp8xJIZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0224bf5043d84158a467780c1b9ee6ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c122fd1ea658494cb2712fb2a6afd21d",
      "placeholder": "​",
      "style": "IPY_MODEL_2896688d51694dc28b0c35b5c1392d52",
      "value": " 1/1 [00:00&lt;00:00, 13.29it/s]"
     }
    },
    "04cdc086b4e84c22b714f29925be7812": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d18beae8ff459fb2d41be9190d3769": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_115878b8291e427b9fb615e330c24276",
       "IPY_MODEL_98243bdfd0be4c9b9029aab54d5f48b1",
       "IPY_MODEL_fe7d0737f6fb470198f4b3977dce2392"
      ],
      "layout": "IPY_MODEL_e2ee0b6b082a407b86c8872d6c5de9fa"
     }
    },
    "06cd16b3378d4effb047b6f0895da170": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "115878b8291e427b9fb615e330c24276": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_544e0369b1d947a3ac9625fb62ec8120",
      "placeholder": "​",
      "style": "IPY_MODEL_58c826569e834b9885f57f120bf38c91",
      "value": "Batches: 100%"
     }
    },
    "2528953881b84d239a4931958e46099c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2896688d51694dc28b0c35b5c1392d52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4825fa9172a74ecd9eefa20c4ae92dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9b91de5957c4ad091c7109d5ec89253",
      "placeholder": "​",
      "style": "IPY_MODEL_486c63fe71e548499e174f5ef63994dc",
      "value": "Batches: 100%"
     }
    },
    "486c63fe71e548499e174f5ef63994dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "544e0369b1d947a3ac9625fb62ec8120": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58c826569e834b9885f57f120bf38c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66e4430fcc034d898adb6cdead5c0eca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83a957d95ad7486485be2e9c51b4b640",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2528953881b84d239a4931958e46099c",
      "value": 1
     }
    },
    "77ed47b807454a48bbbd9781fa7e0195": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83a957d95ad7486485be2e9c51b4b640": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95a400c2c9c44470940e8d1dfa034602": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98243bdfd0be4c9b9029aab54d5f48b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04cdc086b4e84c22b714f29925be7812",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77ed47b807454a48bbbd9781fa7e0195",
      "value": 1
     }
    },
    "9fd1b7427e56400fb098037a39cef39a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4825fa9172a74ecd9eefa20c4ae92dfd",
       "IPY_MODEL_66e4430fcc034d898adb6cdead5c0eca",
       "IPY_MODEL_0224bf5043d84158a467780c1b9ee6ac"
      ],
      "layout": "IPY_MODEL_95a400c2c9c44470940e8d1dfa034602"
     }
    },
    "a9b91de5957c4ad091c7109d5ec89253": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c122fd1ea658494cb2712fb2a6afd21d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2ee0b6b082a407b86c8872d6c5de9fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f61ba415507f4c70b40dd26b4443f0b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe7d0737f6fb470198f4b3977dce2392": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06cd16b3378d4effb047b6f0895da170",
      "placeholder": "​",
      "style": "IPY_MODEL_f61ba415507f4c70b40dd26b4443f0b1",
      "value": " 1/1 [00:00&lt;00:00, 14.14it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
