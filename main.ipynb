{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVnH1uZ2NvdR",
    "outputId": "3ff1c822-1916-4418-824e-fe387912a6b1",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:35:48.420423Z",
     "start_time": "2025-01-09T18:35:30.895886Z"
    }
   },
   "source": [
    "!pip install torch\n",
    "!pip install sentence-transformers\n",
    "!pip install transformers\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install pinecone\n",
    "!pip install pinecone-client\n",
    "!pip install langchain"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.5.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.5)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.0)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (75.6.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.12/site-packages (3.3.1)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.47.1)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\r\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.15.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (0.27.0)\r\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.47.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.16.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.27.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.2.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.2.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.2.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: pinecone in ./.venv/lib/python3.12/site-packages (5.4.2)\r\n",
      "Requirement already satisfied: certifi>=2019.11.17 in ./.venv/lib/python3.12/site-packages (from pinecone) (2024.12.14)\r\n",
      "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone)\r\n",
      "  Obtaining dependency information for pinecone-plugin-inference<4.0.0,>=2.0.0 from https://files.pythonhosted.org/packages/89/45/4ae4e38439919584c2d34b6bef5d0ef8d068030871dd4da911d174840ee6/pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata\r\n",
      "  Using cached pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in ./.venv/lib/python3.12/site-packages (from pinecone) (0.0.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.12/site-packages (from pinecone) (2.9.0.post0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.1 in ./.venv/lib/python3.12/site-packages (from pinecone) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in ./.venv/lib/python3.12/site-packages (from pinecone) (4.12.2)\r\n",
      "Requirement already satisfied: urllib3>=1.26.5 in ./.venv/lib/python3.12/site-packages (from pinecone) (2.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\r\n",
      "Using cached pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\r\n",
      "Installing collected packages: pinecone-plugin-inference\r\n",
      "  Attempting uninstall: pinecone-plugin-inference\r\n",
      "    Found existing installation: pinecone-plugin-inference 1.1.0\r\n",
      "    Uninstalling pinecone-plugin-inference-1.1.0:\r\n",
      "      Successfully uninstalled pinecone-plugin-inference-1.1.0\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pinecone-client 5.0.1 requires pinecone-plugin-inference<2.0.0,>=1.0.3, but you have pinecone-plugin-inference 3.1.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed pinecone-plugin-inference-3.1.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: pinecone-client in ./.venv/lib/python3.12/site-packages (5.0.1)\r\n",
      "Requirement already satisfied: certifi>=2019.11.17 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (2024.12.14)\r\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\r\n",
      "  Obtaining dependency information for pinecone-plugin-inference<2.0.0,>=1.0.3 from https://files.pythonhosted.org/packages/d8/5e/a7eb453cfb3aa9c8c995a1dca5fcf57f79b67400593d5c6759571567e30c/pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata\r\n",
      "  Using cached pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (0.0.7)\r\n",
      "Requirement already satisfied: tqdm>=4.64.1 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (4.12.2)\r\n",
      "Requirement already satisfied: urllib3>=1.26.5 in ./.venv/lib/python3.12/site-packages (from pinecone-client) (2.3.0)\r\n",
      "Using cached pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\r\n",
      "Installing collected packages: pinecone-plugin-inference\r\n",
      "  Attempting uninstall: pinecone-plugin-inference\r\n",
      "    Found existing installation: pinecone-plugin-inference 3.1.0\r\n",
      "    Uninstalling pinecone-plugin-inference-3.1.0:\r\n",
      "      Successfully uninstalled pinecone-plugin-inference-3.1.0\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pinecone 5.4.2 requires pinecone-plugin-inference<4.0.0,>=2.0.0, but you have pinecone-plugin-inference 1.1.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed pinecone-plugin-inference-1.1.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.12/site-packages (0.3.14)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain) (6.0.2)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain) (2.0.36)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain) (3.11.11)\r\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.29)\r\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.5)\r\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in ./.venv/lib/python3.12/site-packages (from langchain) (0.2.10)\r\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in ./.venv/lib/python3.12/site-packages (from langchain) (2.2.1)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.12/site-packages (from langchain) (2.10.4)\r\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain) (2.32.3)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain) (9.0.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.13)\r\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.12.14)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\r\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.8.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7KMfYxvoOAUJ",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:35:55.493371Z",
     "start_time": "2025-01-09T18:35:50.667327Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import csv\n",
    "import random\n",
    "from openai import OpenAI\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksim/PycharmProjects/Advanced-Information-Retrieval/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Cng6SewPXDQP",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:36:32.437972Z",
     "start_time": "2025-01-09T18:36:32.370895Z"
    }
   },
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'api_key'\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhea4Xep6e6C",
    "outputId": "d28b5598-7b14-4137-e621-878f0ee36019",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:37:03.882341Z",
     "start_time": "2025-01-09T18:37:00.204783Z"
    }
   },
   "source": [
    "from pinecone import Pinecone, ServerlessSpec, Index\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"api_key\")\n",
    "\n",
    "index_name = \"song-lyrics-index\"\n",
    "\n",
    "# Check if the index exists; create it if it doesn't\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,  # Dimension of your embeddings\n",
    "        metric=\"cosine\",  # Similarity metric\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "print(\"Pinecone setup complete!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone setup complete!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "naPFLP-SNvdc",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:37:07.806577Z",
     "start_time": "2025-01-09T18:37:07.790424Z"
    }
   },
   "source": [
    "\n",
    "def load_lyrics_dataset(file_path):\n",
    "    try:\n",
    "        # Attempt to read the file with 'latin1' encoding\n",
    "        df = pd.read_csv(file_path, encoding='latin1', on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Preprocessing\n",
    "    # Remove rows with missing values in key columns\n",
    "    df = df.dropna(subset=['Artist', 'Song', 'Lyrics'])\n",
    "    # Remove rows where lyrics have fewer than 3 words\n",
    "    df['lyrics_word_count'] = df['Lyrics'].apply(lambda x: len(str(x).split()))\n",
    "    df = df[df['lyrics_word_count'] >= 3]\n",
    "\n",
    "    df = df.drop(columns=['lyrics_word_count'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# def retrieve_top_k_songs_pinecone(query, index, bi_encoder, k=10):\n",
    "#     query_embedding = bi_encoder.encode([query], convert_to_tensor=False)\n",
    "#     results = index.query(vector=query_embedding.tolist(), top_k=k * 2, include_metadata=True)  # Fetch more to account for duplicates\n",
    "\n",
    "#     # Use a set to keep track of unique songs\n",
    "#     unique_songs = {}\n",
    "#     for match in results.matches:\n",
    "#         key = (match[\"metadata\"][\"Song\"], match[\"metadata\"][\"Artist\"])  # Unique identifier\n",
    "#         if key not in unique_songs:\n",
    "#             unique_songs[key] = {\n",
    "#                 \"Song\": match[\"metadata\"][\"Song\"],\n",
    "#                 \"Artist\": match[\"metadata\"][\"Artist\"],\n",
    "#                 \"Lyric\": match[\"metadata\"][\"Lyric\"],\n",
    "#                 \"score\": match.score,\n",
    "#             }\n",
    "\n",
    "#         # Stop once we have the top k unique songs\n",
    "#         if len(unique_songs) >= k:\n",
    "#             break\n",
    "\n",
    "#     # Sort by score and return top k unique songs\n",
    "#     return sorted(unique_songs.values(), key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "\n",
    "def preprocess_lyrics(lyrics, min_segment_size=3, max_segments=10):\n",
    "    words = lyrics.split()\n",
    "    total_words = len(words)\n",
    "\n",
    "    if total_words <= min_segment_size:\n",
    "        return [lyrics]\n",
    "\n",
    "    segment_size = max(min_segment_size, math.ceil(total_words / max_segments))\n",
    "    segments = [\" \".join(words[i:i+segment_size]) for i in range(0, total_words, segment_size)]\n",
    "    return segments\n",
    "\n",
    "def create_finetuning_dataset(df, num_queries=2, num_negative_pairs=10, min_segment_size=3, max_segments=10, qrels_path=\"qrels.csv\"):\n",
    "    queries = []\n",
    "    corpus = []\n",
    "\n",
    "    with open(qrels_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as qrels_file:\n",
    "        qrels_writer = csv.DictWriter(qrels_file, fieldnames=[\"_query_id\", \"song_id\", \"score\"])\n",
    "        qrels_writer.writeheader()\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            Song = row['Song']\n",
    "            lyrics = row['Lyrics']\n",
    "            artist = row['Artist']\n",
    "\n",
    "            # We split the lyrics into multiple parts and then randomly sample queries from it.\n",
    "            segments = preprocess_lyrics(lyrics, min_segment_size, max_segments)\n",
    "            corpus.append({\"_id\": f\"{idx+1}\", \"Song\": Song, \"lyrics\": lyrics, \"Artist\": artist})\n",
    "\n",
    "            selected_queries = random.sample(segments, min(len(segments), num_queries))\n",
    "            for query in selected_queries:\n",
    "                query_id = f\"q{len(queries)+1}\"\n",
    "                queries.append({\"_query_id\": query_id, \"query\": query})\n",
    "\n",
    "                # The song of origin for the specific query will have label 1 (meaning the query is relevant for that song).\n",
    "                qrels_writer.writerow({\"_query_id\": query_id, \"song_id\": f\"{idx+1}\", \"score\": 1})\n",
    "                # Due to size limitations, we randomly sample 100 songs to set the label to 0 (meaning the query is not relevant for that song).\n",
    "                negative_song_indices = [i for i in range(len(df)) if i != idx]\n",
    "                negative_samples = random.sample(negative_song_indices, num_negative_pairs)\n",
    "\n",
    "                for neg_idx in negative_samples:\n",
    "                    qrels_writer.writerow({\"_query_id\": query_id, \"song_id\": f\"{neg_idx+1}\", \"score\": 0})\n",
    "\n",
    "    return queries, corpus"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E8FpEOKZN-lI",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:37:18.557024Z",
     "start_time": "2025-01-09T18:37:18.261217Z"
    }
   },
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_lyrics(lyrics, chunk_size=100, overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=overlap\n",
    "    )\n",
    "    return splitter.split_text(lyrics)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kbip6bwfFTVq",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:37:33.167526Z",
     "start_time": "2025-01-09T18:37:33.158021Z"
    }
   },
   "source": [
    "def preprocess_and_store_embeddings(data, index, chunk_size=100, overlap=50, batch_size=100):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    bi_encoder = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    rows = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        lyrics = row['Lyrics']\n",
    "        Song = row['Song']\n",
    "        artist = row['Artist']\n",
    "\n",
    "        # Convert lyrics to string and handle potential NaN values\n",
    "        lyrics = str(lyrics)  # Ensure lyrics is a string\n",
    "        if lyrics.lower() == 'nan':\n",
    "            continue\n",
    "\n",
    "        # Use LangChain chunker\n",
    "        chunks = splitter.split_text(lyrics)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            rows.append((f\"{idx}-{i}\", chunk, Song, artist))\n",
    "\n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        batch = rows[i:i+batch_size]\n",
    "\n",
    "        # Extract chunks for embedding\n",
    "        chunks = [row[1] for row in batch]\n",
    "        embeddings = bi_encoder.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "        # Prepare data for upsert\n",
    "        vectors = []\n",
    "        for (vector_id, chunk, Song, artist), embedding in zip(batch, embeddings):\n",
    "            metadata = {\n",
    "                \"Song\": Song,\n",
    "                \"Artist\": artist,\n",
    "                \"Lyrics\": chunk\n",
    "            }\n",
    "            vectors.append((vector_id, embedding.tolist(), metadata))\n",
    "\n",
    "        # Upsert the batch to Pinecone\n",
    "        index.upsert(vectors)\n",
    "        print(f\"Upserted batch {i//batch_size + 1}/{(len(rows) + batch_size - 1) // batch_size}\")\n",
    "\n",
    "    print(\"Embeddings stored in Pinecone!\")\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UPR7hHV0Nvde",
    "ExecuteTime": {
     "end_time": "2025-01-09T18:37:35.840190Z",
     "start_time": "2025-01-09T18:37:35.830392Z"
    }
   },
   "source": [
    "class BiEncoder:\n",
    "    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def encode_texts(self, texts):\n",
    "        \"\"\"Used for encoding lyrics into embeddings.\"\"\"\n",
    "        return self.model.encode(texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "class CrossEncoder:\n",
    "    def __init__(self, model_name='cross-encoder/ms-marco-MiniLM-L-6-v2'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def rank_candidates(self, query, candidates):\n",
    "        inputs = [\n",
    "            self.tokenizer(query, candidate, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "            for candidate in candidates\n",
    "        ]\n",
    "        scores = []\n",
    "        for input_pair in inputs:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(**input_pair).logits\n",
    "            scores.append(logits.item())\n",
    "        ranked_indices = np.argsort(scores)[::-1]\n",
    "        return ranked_indices"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UbcthLP0WCjj",
    "ExecuteTime": {
     "end_time": "2025-01-09T19:02:56.372911Z",
     "start_time": "2025-01-09T19:02:56.366972Z"
    }
   },
   "source": [
    "def refine_query_with_chatgpt(query):\n",
    "    # Few-shot examples to guide the model\n",
    "    example_message = (\n",
    "        \"You are a helpful assistant tasked with extracting and correcting song lyrics \"\n",
    "        \"from user input and identifying the author if mentioned. You should return only the \"\n",
    "        \"lyric portion of the query and refine it for correctness. Format your output as follows:\\n\"\n",
    "        \"query: <start_query>original_query<end_query>\\n\"\n",
    "        \"refined_query: <start_refined_query>refined_lyric<end_refined_query>\\n\"\n",
    "        \"author: <start_author>author_name<end_author>\\n\"\n",
    "        \"If the author is not mentioned, return 'NOT_MENTIONED' for the author.\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"1. User input: \\\"What's the song from Rihanna that goes like 'how about a round of applause'\\\"\\n\"\n",
    "        \"   Output:\\n\"\n",
    "        \"   query: <start_query>how about a round of applause<end_query>\\n\"\n",
    "        \"   refined_query: <start_refined_query>How about a round of applause<end_refined_query>\\n\"\n",
    "        \"   author: <start_author>Rihanna<end_author>\\n\\n\"\n",
    "        \"2. User input: \\\"Twinkle twinkle litl star how I wondr wht u ar\\\"\\n\"\n",
    "        \"   Output:\\n\"\n",
    "        \"   query: <start_query>Twinkle twinkle litl star how I wondr wht u ar<end_query>\\n\"\n",
    "        \"   refined_query: <start_refined_query>Twinkle twinkle little star how I wonder what you are<end_refined_query>\\n\"\n",
    "        \"   author: <start_author>NOT_MENTIONED<end_author>\\n\\n\"\n",
    "        \"3. User input: \\\"song about a broken heart\\\"\\n\"\n",
    "        \"   Output:\\n\"\n",
    "        \"   query: <start_query>a broken heart<end_query>\\n\"\n",
    "        \"   refined_query: <start_refined_query>A broken heart<end_refined_query>\\n\"\n",
    "        \"   author: <start_author>NOT_MENTIONED<end_author>\\n\\n\"\n",
    "        \"Now process this input:\\n\"\n",
    "        f\"User input: {query}\"\n",
    "    )\n",
    "\n",
    "    # Call the OpenAI API\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant tasked with extracting and correcting song lyrics from user input and identifying the author if mentioned.\"},\n",
    "            {\"role\": \"user\", \"content\": example_message},\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    # Extract and format the result\n",
    "    output = chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "    # Manually parse the refined query from the output\n",
    "    start_tag = \"<start_refined_query>\"\n",
    "    end_tag = \"<end_refined_query>\"\n",
    "    refined_query = \"\"\n",
    "    if start_tag in output and end_tag in output:\n",
    "        refined_query = output.split(start_tag)[-1].split(end_tag)[0].strip()\n",
    "\n",
    "    print(\"Refined query:\\n\", refined_query)\n",
    "    return refined_query\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381,
     "referenced_widgets": [
      "04d18beae8ff459fb2d41be9190d3769",
      "115878b8291e427b9fb615e330c24276",
      "98243bdfd0be4c9b9029aab54d5f48b1",
      "fe7d0737f6fb470198f4b3977dce2392",
      "e2ee0b6b082a407b86c8872d6c5de9fa",
      "544e0369b1d947a3ac9625fb62ec8120",
      "58c826569e834b9885f57f120bf38c91",
      "04cdc086b4e84c22b714f29925be7812",
      "77ed47b807454a48bbbd9781fa7e0195",
      "06cd16b3378d4effb047b6f0895da170",
      "f61ba415507f4c70b40dd26b4443f0b1"
     ]
    },
    "id": "Hsv8n1c-Nvdg",
    "outputId": "6b0bbe66-025b-4b03-b290-e04949c7992f",
    "ExecuteTime": {
     "end_time": "2025-01-09T19:00:46.645274Z",
     "start_time": "2025-01-09T19:00:46.639797Z"
    }
   },
   "source": [
    "def song_retrieval_pipeline(query, index, bi_encoder_model='sentence-transformers/all-mpnet-base-v2',\n",
    "                            cross_encoder_model='cross-encoder/ms-marco-MiniLM-L-6-v2', k=5):\n",
    "    # Bi-Encoder\n",
    "    bi_encoder = BiEncoder(bi_encoder_model)\n",
    "\n",
    "    refined_query = refine_query_with_chatgpt(query)\n",
    "    query_embedding = bi_encoder.encode_texts([refined_query])[0].tolist()\n",
    "\n",
    "    # Query Pinecone\n",
    "    results = index.query(vector=query_embedding, top_k=k * 2, include_metadata=True)\n",
    "\n",
    "    # Initial retrieval with duplicate removal\n",
    "    candidates = []\n",
    "    seen_songs = set()\n",
    "\n",
    "    for match in results[\"matches\"]:\n",
    "        song_key = (match[\"metadata\"][\"Song\"], match[\"metadata\"][\"Artist\"])\n",
    "        if song_key not in seen_songs:\n",
    "            seen_songs.add(song_key)\n",
    "            candidates.append({\n",
    "                \"track_name\": match[\"metadata\"][\"Song\"],\n",
    "                \"artist_name\": match[\"metadata\"][\"Artist\"],\n",
    "                \"lyrics_chunk\": match[\"metadata\"][\"Lyrics\"],\n",
    "                \"score\": match.score\n",
    "            })\n",
    "            if len(candidates) >= k:  # Stop if we have enough unique songs\n",
    "                break\n",
    "\n",
    "    lyrics_chunks = [candidate[\"lyrics_chunk\"] for candidate in candidates]\n",
    "\n",
    "    # Cross-Encoder\n",
    "    cross_encoder = CrossEncoder(model_name=cross_encoder_model)\n",
    "    ranked_indices = cross_encoder.rank_candidates(query, lyrics_chunks)\n",
    "\n",
    "    # Re-ranking\n",
    "    re_ranked_songs = [candidates[i] for i in ranked_indices[:k]]\n",
    "    return re_ranked_songs\n",
    "\n",
    "\n",
    "#song_retrieval_pipeline(\"something about an applouse\", index)\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "9fd1b7427e56400fb098037a39cef39a",
      "4825fa9172a74ecd9eefa20c4ae92dfd",
      "66e4430fcc034d898adb6cdead5c0eca",
      "0224bf5043d84158a467780c1b9ee6ac",
      "95a400c2c9c44470940e8d1dfa034602",
      "a9b91de5957c4ad091c7109d5ec89253",
      "486c63fe71e548499e174f5ef63994dc",
      "83a957d95ad7486485be2e9c51b4b640",
      "2528953881b84d239a4931958e46099c",
      "c122fd1ea658494cb2712fb2a6afd21d",
      "2896688d51694dc28b0c35b5c1392d52"
     ]
    },
    "id": "eHmwrHLSNvdh",
    "outputId": "222291d8-8c23-4aaa-d73d-f6be085be7f0",
    "ExecuteTime": {
     "end_time": "2025-01-09T19:03:09.214638Z",
     "start_time": "2025-01-09T19:03:00.943825Z"
    }
   },
   "source": [
    "# Main program\n",
    "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/dataset.csv\"\n",
    "corpus_path = \"/content/drive/MyDrive/Colab Notebooks/corpus.csv\"\n",
    "queries_path = \"/content/drive/MyDrive/Colab Notebooks/queries.csv\"\n",
    "qrels_path = \"/content/drive/MyDrive/Colab Notebooks/qrels.csv\"\n",
    "query = \"songs about broken heart\"\n",
    "\n",
    "if not index.describe_index_stats()[\"total_vector_count\"]:\n",
    "\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(\"Dataset obtained.\")\n",
    "        data = load_lyrics_dataset(dataset_path)\n",
    "\n",
    "        if not (os.path.exists(corpus_path) and os.path.exists(queries_path) and os.path.exists(qrels_path)):\n",
    "            print(\"Required files not found. Generating fine-tuning datasets...\")\n",
    "\n",
    "            # Generate fine-tuning datasets\n",
    "            queries, corpus = create_finetuning_dataset(data, num_queries=5, num_negative_pairs=100, qrels_path=qrels_path)\n",
    "            preprocess_and_store_embeddings(data, index, chunk_size=100, overlap=50)  # Use chunking params\n",
    "\n",
    "            # Save queries and corpus to CSV\n",
    "            queries_df = pd.DataFrame(queries)\n",
    "            corpus_df = pd.DataFrame(corpus)\n",
    "            queries_df.to_csv(queries_path, index=False)\n",
    "            corpus_df.to_csv(corpus_path, index=False)\n",
    "            print(\"Datasets generated and saved.\")\n",
    "        else:\n",
    "            print(\"Datasets already exist. Skipping dataset generation.\")\n",
    "\n",
    "results = song_retrieval_pipeline(query, index)\n",
    "print(\"Top retrieved songs:\")\n",
    "for idx, song in enumerate(results):\n",
    "    print(f\"{idx + 1}. {song['track_name']} by {song['artist_name']}\")\n",
    "    print(f\"Lyrics Chunk: {song['lyrics_chunk']}\\n\")\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined query:\n",
      " Broken heart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top retrieved songs:\n",
      "1. cross my broken heart by the jets\n",
      "Lyrics Chunk: broken heart boy cross my broken heart for you im going to stay this timecross my broken heart boy\n",
      "\n",
      "2. breakeven by the script\n",
      "Lyrics Chunk: when a heart breaks no it dont breakeven even no what am i gonna do when the best part of me was\n",
      "\n",
      "3. two to make it right by seduction\n",
      "Lyrics Chunk: youre suffering from a serious broken heart you know that its a shame but im here to ease your pain\n",
      "\n",
      "4. brokenhearted by brandy featuring wanya morris\n",
      "Lyrics Chunk: brokenhearted its a hurting thing to get over starting all over againonly brokenhearted lifes not\n",
      "\n",
      "5. luv u better by ll cool j\n",
      "Lyrics Chunk: hearts hairbroken and thats hard to fix i had to dig deep inside myself cuz i cant see you bouncin\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2nshSp8xJIZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0224bf5043d84158a467780c1b9ee6ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c122fd1ea658494cb2712fb2a6afd21d",
      "placeholder": "​",
      "style": "IPY_MODEL_2896688d51694dc28b0c35b5c1392d52",
      "value": " 1/1 [00:00&lt;00:00, 13.29it/s]"
     }
    },
    "04cdc086b4e84c22b714f29925be7812": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d18beae8ff459fb2d41be9190d3769": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_115878b8291e427b9fb615e330c24276",
       "IPY_MODEL_98243bdfd0be4c9b9029aab54d5f48b1",
       "IPY_MODEL_fe7d0737f6fb470198f4b3977dce2392"
      ],
      "layout": "IPY_MODEL_e2ee0b6b082a407b86c8872d6c5de9fa"
     }
    },
    "06cd16b3378d4effb047b6f0895da170": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "115878b8291e427b9fb615e330c24276": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_544e0369b1d947a3ac9625fb62ec8120",
      "placeholder": "​",
      "style": "IPY_MODEL_58c826569e834b9885f57f120bf38c91",
      "value": "Batches: 100%"
     }
    },
    "2528953881b84d239a4931958e46099c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2896688d51694dc28b0c35b5c1392d52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4825fa9172a74ecd9eefa20c4ae92dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9b91de5957c4ad091c7109d5ec89253",
      "placeholder": "​",
      "style": "IPY_MODEL_486c63fe71e548499e174f5ef63994dc",
      "value": "Batches: 100%"
     }
    },
    "486c63fe71e548499e174f5ef63994dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "544e0369b1d947a3ac9625fb62ec8120": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58c826569e834b9885f57f120bf38c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66e4430fcc034d898adb6cdead5c0eca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83a957d95ad7486485be2e9c51b4b640",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2528953881b84d239a4931958e46099c",
      "value": 1
     }
    },
    "77ed47b807454a48bbbd9781fa7e0195": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83a957d95ad7486485be2e9c51b4b640": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95a400c2c9c44470940e8d1dfa034602": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98243bdfd0be4c9b9029aab54d5f48b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04cdc086b4e84c22b714f29925be7812",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77ed47b807454a48bbbd9781fa7e0195",
      "value": 1
     }
    },
    "9fd1b7427e56400fb098037a39cef39a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4825fa9172a74ecd9eefa20c4ae92dfd",
       "IPY_MODEL_66e4430fcc034d898adb6cdead5c0eca",
       "IPY_MODEL_0224bf5043d84158a467780c1b9ee6ac"
      ],
      "layout": "IPY_MODEL_95a400c2c9c44470940e8d1dfa034602"
     }
    },
    "a9b91de5957c4ad091c7109d5ec89253": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c122fd1ea658494cb2712fb2a6afd21d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2ee0b6b082a407b86c8872d6c5de9fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f61ba415507f4c70b40dd26b4443f0b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe7d0737f6fb470198f4b3977dce2392": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06cd16b3378d4effb047b6f0895da170",
      "placeholder": "​",
      "style": "IPY_MODEL_f61ba415507f4c70b40dd26b4443f0b1",
      "value": " 1/1 [00:00&lt;00:00, 14.14it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
